{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VY3Ii0gNzelQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import os.path\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, save_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hlthbQrpLvB"
      },
      "source": [
        "# **Training the model to recognize artist's style**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5rImeUifWRo"
      },
      "source": [
        "**Importing Wikiart dataset from Kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7D5IBYrL1yF",
        "outputId": "61a621ce-9627-4c38-c613-c235981d06b6"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!echo '{\"--provide the kaggle token here--\"}' > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download steubk/wikiart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C94a2OGcOxho"
      },
      "outputs": [],
      "source": [
        "!unzip /content/wikiart.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Zfxi-XwIQogv"
      },
      "outputs": [],
      "source": [
        "rm /content/wikiart.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Zsq__4IGBImC"
      },
      "outputs": [],
      "source": [
        "ds_path = \"/content\"\n",
        "df_classes = pd.read_csv(f\"{ds_path}/classes.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fG_lEi4ke4ON"
      },
      "source": [
        "**Preprocessing Function**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jz677Nlx2rVj"
      },
      "outputs": [],
      "source": [
        "def resize(image, size):\n",
        "    image = tf.image.resize(image, [size, size])\n",
        "    return image\n",
        "\n",
        "\n",
        "def preprocess_image(file_path, label, augmentation = True, rescale = True):\n",
        "    img = tf.io.read_file(file_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "\n",
        "    if augmentation:\n",
        "        crop_p = tf.cast( tf.random.uniform([],0,1)<0.75, tf.float32)\n",
        "        r= 1.0 + 0.5*np.random.random()*crop_p\n",
        "\n",
        "\n",
        "        img = resize (img, size=int(IMG_SIZE*r))\n",
        "        img = tf.image.random_crop(\n",
        "            img, size=[IMG_SIZE, IMG_SIZE, 3]\n",
        "        )\n",
        "\n",
        "\n",
        "        img = tf.image.random_flip_left_right(\n",
        "            img\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        img = resize (img, size=IMG_SIZE)\n",
        "\n",
        "    if rescale:\n",
        "        img = tf.cast( img, tf.float32) / 255.0\n",
        "\n",
        "    return img, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTxCn2Hcfhw3"
      },
      "source": [
        "**Splitting the dataset into train, valid and test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "psRLYrGXzg69"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "IMG_SIZE = 300\n",
        "\n",
        "\n",
        "def create_ds(df_classes, ds_path):\n",
        "    filenames = []\n",
        "    labels = []\n",
        "    for i in range(df_classes.shape[0]):\n",
        "      x = df_classes.iloc[i][\"filename\"]\n",
        "      if os.path.exists(f\"{ds_path}/{x}\"):   # If the image with the filename doesnot exist, then it won't be stored in the dataset\n",
        "        filenames += [f\"{ds_path}/{x}\"]\n",
        "        labels += [df_classes.iloc[i][\"artist\"]]\n",
        "    ds = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "    ds = ds.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = configure_ds_for_performance(ds,batch_size, shuffle=True)\n",
        "    return ds\n",
        "\n",
        "def configure_ds_for_performance(ds, batch_size, shuffle, shuffle_buffer_size=32, shuffle_seed=42, reshuffle_each_iteration=True):\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=shuffle_buffer_size, seed=shuffle_seed, reshuffle_each_iteration = reshuffle_each_iteration)\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "df_classes[\"artist\"] = le.fit_transform(df_classes[\"artist\"])   # Encoding the labels to integer values\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(df_classes, df_classes[\"artist\"], test_size=0.3, random_state=42)   # 70% train, 30% validation and test\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)   # 15% validation, 15% test datsets\n",
        "\n",
        "train_ds = create_ds(X_train, ds_path)\n",
        "\n",
        "valid_ds = create_ds(X_val, ds_path)\n",
        "\n",
        "test_ds = create_ds(X_test, ds_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEdxryeRfuDn"
      },
      "source": [
        "**Some samples of the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "YOAJz1mFUJMJ"
      },
      "outputs": [],
      "source": [
        "def show_samples (df_train, ds_path):\n",
        "\n",
        "    df = df_train.sample(n=50, random_state=421)\n",
        "\n",
        "    filenames = [ f\"{ds_path}/{filename}\" for filename in  df[\"filename\"].values if os.path.exists(f\"{ds_path}/{filename}\")]\n",
        "    labels = df[\"artist\"]\n",
        "    plt.figure(figsize=(30, 30))\n",
        "    for n, (filename, label) in enumerate(zip(filenames,labels)):\n",
        "        if n < 10*10:\n",
        "            image, label = preprocess_image(filename, label, augmentation=False, rescale = False)\n",
        "\n",
        "            ax = plt.subplot(10, 10, n+1)\n",
        "            plt.imshow(image.numpy().astype(\"uint8\"))\n",
        "            ax.set_title( label )\n",
        "            plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "show_samples(X_train, ds_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycW9YS41gU-P"
      },
      "source": [
        "**Some images after preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoQuCBmWUV0v"
      },
      "outputs": [],
      "source": [
        "def show_augmentation (df_train, ds_path):\n",
        "\n",
        "    df = df_train.sample(n=100, random_state=421)\n",
        "    filenames = [ f\"{ds_path}/{filename}\" for filename in  df[\"filename\"].values if os.path.exists(f\"{ds_path}/{filename}\")]\n",
        "    labels = df[\"description\"].values\n",
        "\n",
        "    plt.figure(figsize=(15, 30))\n",
        "    for n, (filename, label) in enumerate(zip(filenames,labels)):\n",
        "        if n < 10:\n",
        "            image, label = preprocess_image(filename, label, augmentation=False, rescale = False)\n",
        "            ax = plt.subplot(10, 5, 5*n + 1)\n",
        "\n",
        "            ax.imshow(image.numpy().astype(\"uint8\"))\n",
        "            ax.set_title( label )\n",
        "            ax.axis(\"off\")\n",
        "            for i in range(4):\n",
        "                ax = plt.subplot(10, 5, 5*n + 2+i)\n",
        "                image_aug, label = preprocess_image(filename, label, augmentation=True, rescale = False)\n",
        "                ax.imshow(image_aug.numpy().astype(\"uint8\"))\n",
        "                ax.axis(\"off\")\n",
        "\n",
        "show_augmentation(X_train, ds_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFD620HagkQM"
      },
      "source": [
        "**Building a model using EfficientNetB3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "matqKCffca4u",
        "outputId": "9be88d97-1104-4343-ce1d-2f6f66c02db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3.h5\n",
            "50095040/50095040 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "base_model = tf.keras.applications.EfficientNetB3(weights='imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "dB35R6UWbMP2"
      },
      "outputs": [],
      "source": [
        "last_output = base_model.output\n",
        "\n",
        "x = layers.Flatten()(last_output)\n",
        "x = layers.Dense(512, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(1119, activation=\"softmax\")(x)   # In the dataset, number of artists = 1119\n",
        "\n",
        "model = Model(base_model.input, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "pbsfJlSfznnE"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBjKjsXAg5XU"
      },
      "source": [
        "**Training the model using train and validation datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dITBnsLnzrJZ"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_ds, epochs=10, validation_data = valid_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV8DQkrdhOsW"
      },
      "source": [
        "**Testimg the model and visualising the performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjwyuIOQzuI4"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWG9MsPPr4XQ"
      },
      "source": [
        "# **Transfering the art style**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRNwu-n3ou36"
      },
      "source": [
        "**Extracting outputs of intermediate layers of the model to extract style from the art**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR0LONDpheRN"
      },
      "outputs": [],
      "source": [
        "for i, layer in enumerate(base_model.layers):\n",
        "   print(i, layer.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMn7uLZ8h8mM"
      },
      "outputs": [],
      "source": [
        "style_layers = ['block1a_dwconv',\n",
        "                'block1b_dwconv',\n",
        "                'block2a_expand_conv',\n",
        "                'block2b_expand_conv',\n",
        "                'block2c_expand_conv',\n",
        "                'block3a_expand_conv',\n",
        "                'block3b_expand_conv',\n",
        "                'block3c_expand_conv',\n",
        "                'block4a_expand_conv',\n",
        "                'block4b_expand_conv',\n",
        "                'block4c_expand_conv',\n",
        "                'block4d_expand_conv',\n",
        "                'block4e_expand_conv',\n",
        "                'block5a_expand_conv',\n",
        "                'block5b_expand_conv',\n",
        "                'block5c_expand_conv',\n",
        "                'block5d_expand_conv',\n",
        "                'block5e_expand_conv',\n",
        "                'block6a_expand_conv',\n",
        "                'block6b_expand_conv',\n",
        "                'block6c_expand_conv',\n",
        "                'block6d_expand_conv',\n",
        "                'block6e_expand_conv',\n",
        "                'block6f_expand_conv',\n",
        "                'block7a_expand_conv',\n",
        "                'block7b_expand_conv']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKlrXxdsl_G9"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_image(image_path, target_size=(256, 256)):\n",
        "    img = load_img(image_path, target_size=target_size)\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    img_array /= 255.0\n",
        "    return img_array\n",
        "\n",
        "def build_model(style_path, content_shape=(256, 256, 3)):\n",
        "    style_image = load_and_preprocess_image(style_path)\n",
        "\n",
        "    model = Model.Sequential()\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=content_shape))\n",
        "\n",
        "    style_outputs = [layer.output for layer in style_layers]\n",
        "\n",
        "\n",
        "    style_model = Model([model.input], style_outputs)\n",
        "\n",
        "\n",
        "    style_features = style_model(style_image)\n",
        "\n",
        "    for layer in style_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.add(layers.Lambda(lambda x: x * 0.8))\n",
        "    model.add(layers.Lambda(lambda x: x + style_features))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GstUPM32qDq-"
      },
      "source": [
        "**Functions to calculate Loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUz4kdE9puvv"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(input_tensor):\n",
        "    channels = int(input_tensor.shape[-1])\n",
        "    a = tf.reshape(input_tensor, [-1, channels])\n",
        "    n = tf.shape(a)[0]\n",
        "    gram = tf.matmul(a, a, transpose_a=True)\n",
        "    return gram / tf.cast(n, tf.float32)\n",
        "\n",
        "def content_loss(content, generated):\n",
        "    return tf.reduce_mean(tf.square(content - generated))\n",
        "\n",
        "def style_loss(style, generated):\n",
        "    style_gram = gram_matrix(style)\n",
        "    generated_gram = gram_matrix(generated)\n",
        "    return tf.reduce_mean(tf.square(style_gram - generated_gram))\n",
        "\n",
        "def total_variation_loss(image):\n",
        "    x_deltas, y_deltas = image[:, 1:, :, :] - image[:, :-1, :, :], image[:, :, 1:, :] - image[:, :, :-1, :]\n",
        "    return tf.reduce_mean(tf.square(x_deltas) + tf.square(y_deltas))\n",
        "\n",
        "def total_loss(content, style, generated, content_weight=1e3, style_weight=1e-2, tv_weight=1e-2):\n",
        "    content_loss_value = content_weight * content_loss(content, generated)\n",
        "    style_loss_value = style_weight * style_loss(style, generated)\n",
        "    tv_loss_value = tv_weight * total_variation_loss(generated)\n",
        "    return content_loss_value + style_loss_value + tv_loss_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFkGmoZ_rfn7"
      },
      "source": [
        "**Transfering the style from style image to original image(content image)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pO02jIDKpzmj"
      },
      "outputs": [],
      "source": [
        "def style_transfer(content_path, style_path, output_path, iterations=1000):\n",
        "\n",
        "    content_image = load_and_preprocess_image(content_path)\n",
        "\n",
        "    model = build_model(style_path, content_image.shape[1:])\n",
        "\n",
        "    opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
        "    for i in range(iterations):\n",
        "        with tf.GradientTape() as tape:\n",
        "            generated_image = model(content_image)\n",
        "            loss = total_loss(content_image, style_image, generated_image)\n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        opt.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(\"Iteration {}: Loss: {}\".format(i, loss))\n",
        "\n",
        "    generated_image = tf.squeeze(generated_image, 0)\n",
        "    generated_image = tf.clip_by_value(generated_image, 0, 1)\n",
        "    generated_image = tf.image.convert_image_dtype(generated_image, dtype=tf.uint8)\n",
        "    save_img(output_path, generated_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSzo151crAyu"
      },
      "source": [
        "**An example for transfering the style and the visualisation of the result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBZkzn9tp2Nd"
      },
      "outputs": [],
      "source": [
        "style_path = '/content/abidin-dino_drawing-pain-1968.jpg'\n",
        "content_path = '/content/download.jpg'\n",
        "content_image = load_img(content_path)\n",
        "style_image = load_img(style_path)\n",
        "output_path = 'output_image.jpg'\n",
        "style_transfer(content_path, style_path, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qldtsYFxql7B"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(30,30))\n",
        "plt.subplot(5,5,1)\n",
        "plt.title(\"Base Image\",fontsize=20)\n",
        "img_original = load_img(content_path)\n",
        "plt.imshow(img_original)\n",
        "\n",
        "plt.subplot(5,5,1+1)\n",
        "plt.title(\"Style Image\",fontsize=20)\n",
        "img_style = load_img(style_path)\n",
        "plt.imshow(img_style)\n",
        "\n",
        "plt.subplot(5,5,1+2)\n",
        "plt.title(\"Final Image\",fontsize=20)\n",
        "imgx = load_img(output_path)\n",
        "plt.imshow(imgx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **LIMITATIONS**\n",
        "\n",
        " \n",
        "\n",
        "*   Testing the other EfficientNet models and selecting the best one might help make the model training better.\n",
        "*   Using other metrics along with accuracy might make the model more efficient.\n",
        "*   More preproccesing functions can be used on the art image to extract the style features more effectively.\n",
        "*   Training the model on a subset of the Wikiart dataset will help running the code more faster since the dataset is too large."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
